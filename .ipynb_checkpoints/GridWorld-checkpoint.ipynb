{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf \n",
    "import os\n",
    "%matplotlib inline\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(object):\n",
    "    def __init__(self, coordinates, size, intensity, channel, reward, name):\n",
    "        self.x, self.y = coordinates[0], coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld():\n",
    "    def __init__(self, size):\n",
    "        self.x_size = size\n",
    "        self.y_size = size\n",
    "        self.action_num = 4\n",
    "        self.block_num = 7\n",
    "        self.blocks = None\n",
    "        self.available_grids = None\n",
    "        self.state = None\n",
    "\n",
    "    def get_positions(self, num):\n",
    "        indices = np.random.choice(np.arange(len(self.available_grids)), size=num, replace=False)\n",
    "        grids = [self.available_grids[index] for index in indices]\n",
    "        for grid in grids:\n",
    "            self.available_grids.remove(grid)\n",
    "        return grids\n",
    "\n",
    "    def reset(self):\n",
    "        self.available_grids = [(x, y) for x in range(self.x_size) for y in range(self.y_size)]\n",
    "        postions = self.get_positions(self.block_num)\n",
    "        size = [1] * self.block_num\n",
    "        intensity = [1] * self.block_num\n",
    "        channel = [2, 1, 0, 1, 0, 1, 1]\n",
    "        reward = [None, 1, -1, 1, -1, 1, 1]\n",
    "        name = [\"hero\", \"goal\", \"fire\", \"goal\", \"fire\", \"goal\", \"goal\"]\n",
    "        self.blocks = [Block(*args) for args in zip(postions, size, intensity, channel, reward, name)]\n",
    "        self.state = self.render()\n",
    "        return self.state\n",
    "    \n",
    "    def move(self, direction):\n",
    "        hero = self.blocks[0]\n",
    "        if direction == 0 and hero.y >=1:\n",
    "            hero.y -= 1\n",
    "        elif direction == 1 and hero.y <= self.y_size - 2:\n",
    "            hero.y += 1\n",
    "        elif direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        elif direction == 3 and hero.x <= self.x_size - 2:\n",
    "            hero.x += 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def check_hit(self):\n",
    "        hero = self.blocks.pop(0)\n",
    "        for block in self.blocks:\n",
    "            if (hero.x == block.x) and (hero.y == block.y):\n",
    "                self.available_grids.append([block.x, block.y])\n",
    "                self.blocks.remove(block)\n",
    "                if block.name == \"goal\":\n",
    "                    self.blocks.append(Block(*self.get_positions(1), 1, 1, 1, 1, \"goal\"))\n",
    "                elif block.name == \"fire\":\n",
    "                    self.blocks.append(Block(*self.get_positions(1), 1, 1, 0, -1, \"fire\"))\n",
    "                else:\n",
    "                    raise\n",
    "                self.blocks.insert(0, hero)\n",
    "                return block.reward, False\n",
    "        \n",
    "        self.blocks.insert(0, hero)\n",
    "        return 0.0, False\n",
    "\n",
    "    def render(self):\n",
    "        canvas = np.ones([self.y_size+2, self.x_size+2, 3])\n",
    "        canvas[1:-1,1:-1,:] = 0\n",
    "        for block in self.blocks:\n",
    "            canvas[block.y+1:block.y+block.size+1,block.x+1:block.x+block.size+1,block.channel] = block.intensity\n",
    "        r = scipy.misc.imresize(canvas[:,:,0], [84,84,1], interp='nearest')\n",
    "        g = scipy.misc.imresize(canvas[:,:,1], [84,84,1], interp='nearest')\n",
    "        b = scipy.misc.imresize(canvas[:,:,2], [84,84,1], interp='nearest')\n",
    "        return np.stack([r, g, b],axis=2)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.move(action)\n",
    "        reward, done = self.check_hit()\n",
    "        self.state = self.render()\n",
    "        return self.state, reward, done\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.imshow(self.state, interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld(size=5)\n",
    "_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD/CAYAAADRymv0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA9CdxgsUTt5tAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fda370b49b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD/CAYAAADRymv0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvllCDr5ZQv8PUp/FC07qBQgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fda337ea9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, env, name):\n",
    "        self.name = name\n",
    "        self.env = env\n",
    "        ################## 网络结构 ##############\n",
    "        self.scalar_input =  tf.placeholder(shape=[None,21168],dtype=tf.float32)\n",
    "        self.image = tf.reshape(self.scalar_input,shape=[-1,84,84,3])\n",
    "        self.conv1 = tf.contrib.layers.convolution2d(inputs=self.image, num_outputs=32, \n",
    "            kernel_size=[8,8], stride=[4,4], padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d(inputs=self.conv1, num_outputs=64, \n",
    "            kernel_size=[4,4], stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d(inputs=self.conv2, num_outputs=64, \n",
    "            kernel_size=[3,3], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d(inputs=self.conv3, num_outputs=512, \n",
    "            kernel_size=[7,7], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "\n",
    "        self.conv4_1, self.conv4_2 = tf.split(self.conv4, 2, 3)\n",
    "        self.flat_1 = tf.contrib.layers.flatten(self.conv4_1)\n",
    "        self.flat_2 = tf.contrib.layers.flatten(self.conv4_2)\n",
    "        self.advantage_weight = tf.Variable(tf.random_normal([256, self.env.action_num]), name=\"advantage_weight\")\n",
    "        self.value_weight = tf.Variable(tf.random_normal([256, 1]), name=\"value_weight\")\n",
    "        self.advantage = tf.matmul(self.flat_1, self.advantage_weight)\n",
    "        self.value = tf.matmul(self.flat_2, self.value_weight)\n",
    "        \n",
    "        self.q_value = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, reduction_indices=1, keep_dims=True))\n",
    "        self.action = tf.argmax(self.q_value, 1)\n",
    "\n",
    "        # self.actions 是一维的，类似于[1,1,2,3,1,0.....]\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        # self.target_q 是一维的，是self.actions 中每个动作对应的q值\n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        # self.q_value_of_action 是一维的，是self.actions 中每个动作对应的预测q值\n",
    "        self.actions_onehot = tf.one_hot(self.actions, self.env.action_num, dtype=tf.float32)\n",
    "        self.q_value_on_action = tf.reduce_sum(tf.multiply(self.q_value, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.target_q - self.q_value_on_action))\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.00001)\n",
    "        self.update_model = self.trainer.minimize(self.loss)\n",
    "\n",
    "class ExperienceBuffer():\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        num_to_remove = len(self.buffer) + len(experience) - self.buffer_size\n",
    "        if num_to_remove > 0:\n",
    "            for i in range(num_to_remove):\n",
    "                self.buffer.pop(0)\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self, size):\n",
    "        indices = np.random.choice(np.arange(len(self.buffer)), size=size)\n",
    "        return [self.buffer[index] for index in indices]\n",
    "\n",
    "     \n",
    "# def get_update_target_ops(variables, tau):\n",
    "#     main_DQN_vars = variables[0:len(variables)//2]\n",
    "#     target_DQN_vars = variables[len(variables)//2:]\n",
    "#     ops = []\n",
    "#     for i in range(len(target_DQN_vars)):\n",
    "#         ops.append(target_DQN_vars[i].assign(\n",
    "#             main_DQN_vars[i].value() * tau + (1 - tau) * target_DQN_vars[i].value()\n",
    "#             ))\n",
    "#     return ops\n",
    "\n",
    "def get_update_target_ops(tfVars, tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def update_target(ops, sess):\n",
    "    for op in ops:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "#How often to perform a training step.\n",
    "update_freq = 4\n",
    "gamma = .99\n",
    "random_upper_bound = 1\n",
    "random_lower_bound = 0.1\n",
    "annealing_steps = 10000\n",
    "random_threshold = random_upper_bound\n",
    "drop_step = (random_upper_bound - random_lower_bound) / annealing_steps\n",
    "\n",
    "#How many episodes of game environment to train network with.\n",
    "num_episodes = 10000\n",
    "#How many steps of random actions before training begins.\n",
    "pre_train_steps = 10000\n",
    "#The max allowed length for one episode.\n",
    "max_episode_length = 50\n",
    "load_model = False \n",
    "path = \"./dqn\"\n",
    "#Rate to update target network toward primary network\n",
    "tau = 0.001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "main_DQN = DQN(env, \"main\")\n",
    "target_DQN = DQN(env, \"target\")\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "update_target_ops_1 = get_update_target_ops(tf.trainable_variables(), 1)\n",
    "update_target_ops_2 = get_update_target_ops(tf.trainable_variables(), tau)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(init_op)\n",
    "#     update_target(update_target_ops_1, sess)\n",
    "#     observation = env.reset()\n",
    "#     observation = np.reshape(observation, [21168])\n",
    "#     print(sess.run(target_DQN.q_value, feed_dict={target_DQN.scalar_input: [observation]}))\n",
    "#     print(sess.run(main_DQN.action, feed_dict={main_DQN.scalar_input: [observation]}))\n",
    "#     print(sess.run(main_DQN.actions_onehot, feed_dict={main_DQN.actions: [2]}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 25 , average reward of last 25 episode 1.96\n",
      "episode 50 , average reward of last 25 episode 1.72\n",
      "episode 75 , average reward of last 25 episode 2.68\n",
      "episode 100 , average reward of last 25 episode 2.08\n",
      "episode 125 , average reward of last 25 episode 2.32\n",
      "episode 150 , average reward of last 25 episode 2.28\n",
      "episode 175 , average reward of last 25 episode 2.16\n",
      "episode 200 , average reward of last 25 episode 1.68\n",
      "episode 225 , average reward of last 25 episode 2.0\n",
      "episode 250 , average reward of last 25 episode 1.32\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    #Set the target network to be equal to the main network.\n",
    "    update_target(update_target_ops_2, sess)\n",
    "    global_experience_buffer = ExperienceBuffer()\n",
    "    \n",
    "    #create list to contain total rewards per episode\n",
    "    total_reward_list = []\n",
    "    steps = 0\n",
    "    for i in range(num_episodes + 1):\n",
    "        episode_buffer = ExperienceBuffer()\n",
    "        observation = env.reset()\n",
    "        observation = np.reshape(observation, [21168])\n",
    "        done = False\n",
    "        total_reward_in_episode = 0\n",
    "        steps_in_episode = 0\n",
    "        while steps_in_episode < max_episode_length:\n",
    "            # 积累样本\n",
    "            steps_in_episode += 1\n",
    "            steps += 1\n",
    "            if np.random.rand(1) < random_threshold or steps < pre_train_steps:\n",
    "                action = np.random.randint(0, 4)\n",
    "            else:\n",
    "                action = sess.run(main_DQN.action, feed_dict={main_DQN.scalar_input: [observation]})[0]\n",
    "            new_observation, reward, done = env.step(action)\n",
    "            new_observation = np.reshape(new_observation, [21168])\n",
    "            #Save the experience to episode buffer\n",
    "            episode_buffer.add([[observation, action, reward, new_observation, done]])\n",
    "            \n",
    "            total_reward_in_episode += reward\n",
    "            observation = new_observation\n",
    "            # 积累了 pre_train_steps/max_episode_length 次经历的pre_train_steps个样本之后才会第一次开始衰减随机门限，进行第一次训练。\n",
    "            if steps >= pre_train_steps:\n",
    "                if random_threshold > random_lower_bound:\n",
    "                    random_threshold -= drop_step\n",
    "                if steps % (update_freq) == 0:\n",
    "                    train_batch = global_experience_buffer.sample(batch_size)\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    observations = np.vstack([record[0] for record in train_batch])\n",
    "                    actions = np.array([record[1] for record in train_batch])\n",
    "                    instant_rewards = np.array([record[2] for record in train_batch])\n",
    "                    new_observations = np.vstack([record[3] for record in train_batch])\n",
    "                    \n",
    "                    action = sess.run(main_DQN.action, feed_dict={main_DQN.scalar_input: new_observations})\n",
    "                    q_value, value, advantage = sess.run([target_DQN.q_value, target_DQN.value, target_DQN.advantage], feed_dict={target_DQN.scalar_input: new_observations})\n",
    "                    labels = instant_rewards + gamma * q_value[range(batch_size) ,action]\n",
    "                    # print(q_value, advantage)\n",
    "                    # Update the network with our target values.\n",
    "                    _ = sess.run(main_DQN.update_model, feed_dict={\n",
    "                        main_DQN.scalar_input: observations,\n",
    "                        main_DQN.target_q: labels,\n",
    "                        main_DQN.actions: actions})\n",
    "\n",
    "                    #update the target network towards the main network.\n",
    "                    update_target(update_target_ops_2, sess)\n",
    "        \n",
    "        #Get all experiences from this episode\n",
    "        global_experience_buffer.add(episode_buffer.buffer)\n",
    "        total_reward_list.append(total_reward_in_episode)\n",
    "        \n",
    "        if i>0 and i % 25 == 0:\n",
    "            print('episode',i,', average reward of last 25 episode', np.mean(total_reward_list[-25:]))\n",
    "        #Periodically save the model.\n",
    "        if i>0 and i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")            \n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
